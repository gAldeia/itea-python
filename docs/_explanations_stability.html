

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Explanations Stability &mdash; Interaction-Transformation Evolutionary Algorithm</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Integrating with the Scikit: Pipeline and Gridsearch" href="_integrating_with_scikits_classes.html" />
    <link rel="prev" title="Using agnostic explainers" href="_agnostic_explainers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> ITEA: Interaction-Transformation Evolutionary Algorithm
          

          
            
            <img src="_static/itea-logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gettingstarted.html">Installing</a></li>
<li class="toctree-l1"><a class="reference internal" href="gettingstarted.html#minimal-examples">Minimal examples</a></li>
</ul>
<p class="caption"><span class="caption-text">ITEA package documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="base.html">itea._base</a></li>
<li class="toctree-l1"><a class="reference internal" href="itea.regression.html">itea.regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="itea.classification.html">itea.classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="itea.inspection.html">itea.inspection</a></li>
</ul>
<p class="caption"><span class="caption-text">Example notebooks:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="_agnostic_explainers.html">Using agnostic explainers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Explanations Stability</a></li>
<li class="toctree-l1"><a class="reference internal" href="_integrating_with_scikits_classes.html">Integrating with the Scikit: Pipeline and Gridsearch</a></li>
<li class="toctree-l1"><a class="reference internal" href="_interacting_with_protodash.html">Interacting with ProtoDash</a></li>
<li class="toctree-l1"><a class="reference internal" href="_multiclass_example.html">Iris classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="_regression_example.html">California housing regression</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ITEA: Interaction-Transformation Evolutionary Algorithm</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Explanations Stability</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/_explanations_stability.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Explanations-Stability">
<h1>Explanations Stability<a class="headerlink" href="#Explanations-Stability" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, we will explore how to calculate the stability of a local explanation for the Partial Effects explainer. We’ll first start creating a classifier and fitting it with the iris data set. Then, we’ll define the stability metric function and see how stable are the explanations.</p>
<p>This example was inspired by the usage of the stability metric in <em>Plumb, Gregory, Maruan, Al-Shedivat, Ángel Alexander, Cabrera, Adam, Perer, Eric, Xing, and Ameet, Talwalkar. “Regularizing Black-box Models for Improved Interpretability.” . Advances in Neural Information Processing Systems (pp. 10526–10536). Curran Associates, Inc., 2020.</em></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span>  <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># automatically differentiable implementation of numpy</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span> <span class="c1"># v0.2.13</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">IPython.display</span>         <span class="kn">import</span> <span class="n">display</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">itea.classification</span> <span class="kn">import</span> <span class="n">ITEA_classifier</span>
<span class="kn">from</span> <span class="nn">itea.inspection</span>     <span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;itea&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let us start by loading the data set and fitting a simple classifier. Latter, the classifier will be used to explain a single instance and the stability of the explanation will be calculated for different sizes of neighborhoods.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">iris_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span>      <span class="o">=</span> <span class="n">iris_data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="n">labels</span>    <span class="o">=</span> <span class="n">iris_data</span><span class="p">[</span><span class="s1">&#39;feature_names&#39;</span><span class="p">]</span>
<span class="n">targets</span>   <span class="o">=</span> <span class="n">iris_data</span><span class="p">[</span><span class="s1">&#39;target_names&#39;</span><span class="p">]</span>

<span class="c1"># changing numbers to the class names</span>
<span class="n">y_targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">targets</span><span class="p">[</span><span class="n">yi</span><span class="p">]</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">y</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y_targets</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Creating transformation functions for ITEA using jax.numpy</span>
<span class="c1"># (so we don&#39;t need to analytically calculate its derivatives)</span>
<span class="n">tfuncs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;id&#39;</span>       <span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="s1">&#39;sqrt.abs&#39;</span> <span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span>
    <span class="s1">&#39;log&#39;</span>      <span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">,</span>
    <span class="s1">&#39;exp&#39;</span>      <span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span>
<span class="p">}</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">ITEA_classifier</span><span class="p">(</span>
    <span class="n">gens</span>            <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">popsize</span>         <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">max_terms</span>       <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">expolim</span>         <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">verbose</span>         <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">tfuncs</span>          <span class="o">=</span> <span class="n">tfuncs</span><span class="p">,</span>
    <span class="n">labels</span>          <span class="o">=</span> <span class="n">labels</span><span class="p">,</span>
    <span class="n">simplify_method</span> <span class="o">=</span> <span class="s1">&#39;simplify_by_var&#39;</span><span class="p">,</span>
    <span class="n">random_state</span>    <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
gen | smallest fitness | mean fitness | highest fitness | remaining time (s)
----------------------------------------------------------------------------
  0 |         0.560000 |     0.848200 |        0.970000 | 0min17seg
  5 |         0.740000 |     0.961800 |        0.980000 | 0min12seg
 10 |         0.790000 |     0.969400 |        0.990000 | 0min13seg
 15 |         0.950000 |     0.982200 |        0.990000 | 0min10seg
 20 |         0.890000 |     0.982200 |        0.990000 | 0min10seg
 25 |         0.860000 |     0.982400 |        0.990000 | 0min11seg
 30 |         0.930000 |     0.981800 |        0.990000 | 0min6seg
 35 |         0.960000 |     0.982600 |        0.990000 | 0min5seg
 40 |         0.940000 |     0.981600 |        0.990000 | 0min3seg
 45 |         0.660000 |     0.973800 |        0.990000 | 0min2seg
</pre></div></div>
</div>
<section id="Local-explanations-with-the-Partial-Effects">
<h2>Local explanations with the Partial Effects<a class="headerlink" href="#Local-explanations-with-the-Partial-Effects" title="Permalink to this headline">¶</a></h2>
<p>To calculate the local explanations, we can use the function <code class="docutils literal notranslate"><span class="pre">ITExpr_explainer.average_partial_effects</span></code>, passing as an argument a single sample we want to explain.</p>
<p>We can see the explanation both visually or as an array with the importance for each feature.</p>
<p>Let’s start by creating an explainer instance.</p>
<p><strong>NOTE</strong>: In the remaining of this notebook, all local explanations will be calculated over the first element of the test set.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">ITExpr_explainer</span><span class="p">(</span>
    <span class="n">itexpr</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">bestsol_</span><span class="p">,</span>
    <span class="n">tfuncs</span><span class="o">=</span><span class="n">tfuncs</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The visual inspection of feature importances can be useful in some cases: * The features are ordered by the most important to the least important feature; * The proportions between feature importances allows having a more clear understanding about its magnitudes; * We can see how much each class is using each feature to make its classification.</p>
<p>It is important to notice that the prediction of the model is based on the class with the highest probability from the decision function. The decision function is a logit function using the IT expression as its linear model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">explainer</span><span class="o">.</span><span class="n">plot_feature_importances</span><span class="p">(</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">importance_method</span> <span class="o">=</span> <span class="s1">&#39;pe&#39;</span><span class="p">,</span>
    <span class="n">grouping_threshold</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">target</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">barh_kw</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;edgecolor&#39;</span> <span class="p">:</span> <span class="s1">&#39;k&#39;</span><span class="p">},</span>
    <span class="n">show</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/_explanations_stability_7_0.png" src="_images/_explanations_stability_7_0.png" />
</div>
</div>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">average_partial_effects</span></code> directly to obtain an array with numeric explanations, making it easy to manipulate the values to assess stabilities.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">explainer</span><span class="o">.</span><span class="n">average_partial_effects</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[6.85112219e-03, 5.00492617e-02, 4.76003440e-02, 1.51608147e-01],
       [2.74413002e-04, 3.40860669e-03, 1.31835324e-03, 6.55848291e-03],
       [1.03778588e-01, 6.74201575e-01, 6.71034697e-01, 2.10067814e+00]])
</pre></div></div>
</div>
<p>The function returns an array, where each line corresponds to the importance for each class. As we can see from the bar plot, the total importance for a given feature is the summation of the importance for each class:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="n">explainer</span><span class="o">.</span><span class="n">average_partial_effects</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([0.11090412, 0.72765944, 0.71995339, 2.25884477])
</pre></div></div>
</div>
</section>
<section id="Stability-of-an-explanation">
<h2>Stability of an explanation<a class="headerlink" href="#Stability-of-an-explanation" title="Permalink to this headline">¶</a></h2>
<p>In the mentioned paper, the authors discuss metrics to evaluate model agnostic explainers.</p>
<p>The stability measures how much the explanation changes when the feature slightly changes, where lower values are better. Higher values imply that, when the feature being explained changes in a small proportion, the feature importances responds in a much larger proportion, then the explainer is not reliable since it is not locally stable, and cannot be trusted for the given instance.</p>
<p>Let <img class="math" src="_images/math/dc72c8b3d50b5d8f8e1059b84df06ba2627ac1a7.png" alt="f:\mathbf{x} \rightarrow y"/>, with <img class="math" src="_images/math/6c49d5774c7a67ba3ecda7bbd7fb5637ede032e2.png" alt="\mathbf{x} = \{x_0, x_1, ..., x_n\}"/> be a model we want to explain, and <img class="math" src="_images/math/9b1df625ee0eb6ba423bf6516916201700d4e93d.png" alt="g: (f, \mathbf{x}_i) \rightarrow \mathbf{e}"/> a explainer that takes a model and a instance to explain and attributes one feature importance for each <img class="math" src="_images/math/9153b1ceb005926ad936db10b06fb2d5e3d90135.png" alt="x \in \mathbf{x}_i"/>.</p>
<p>The stability of the explanation is calculated by:</p>
<div class="math">
<p><img src="_images/math/dc424c45c0feedad7fe20589a263237a550b07e3.png" alt="S(f, g, \mathbf{x}_i, \mathbf{N}_{\mathbf{x}_i}) = \mathbb{E}_{\mathbf{x}_j \sim \mathbf{N}_{\mathbf{x}_i}} \left [ || g(\mathbf{x}_i, f) - g(\mathbf{x}_j, f)||_{2}^{2} \right ]."/></p>
</div><p>In other words, the stability function evaluates the mean distance between the explanation for the original input and all sampled neighbors.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">explain_single</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;wrapping the process to explain a single instance.</span>

<span class="sd">    The input x should be a single observation of shape (n_features, ).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">explainer</span><span class="o">.</span><span class="n">average_partial_effects</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">norm_p2</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;p2 norm of a vector.</span>

<span class="sd">    the vector should be an array of shape (n, ).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>


<span class="k">def</span> <span class="nf">stability</span><span class="p">(</span><span class="n">explainer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">neighborhood</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stability function.</span>

<span class="sd">    Takes as argument an explanation method, a single observation</span>
<span class="sd">    x of shape (n_features, ), and the neighborhood as a matrix of</span>
<span class="sd">    shape (n_neighbors, n_features), where each line is a sampled</span>
<span class="sd">    neighbor and each column is the feature value of the sample.</span>

<span class="sd">    Returns the mean squared p2-norm of the difference between the</span>
<span class="sd">    original explanation and every sampled neighbor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">original_explanation</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">([</span>
        <span class="n">norm_p2</span><span class="p">(</span><span class="n">explainer</span><span class="p">(</span><span class="n">nb</span><span class="p">)</span> <span class="o">-</span> <span class="n">original_explanation</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">for</span> <span class="n">nb</span> <span class="ow">in</span> <span class="n">neighborhood</span>
    <span class="p">])</span>


<span class="k">def</span> <span class="nf">neighborhood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Method to create samples around a given observation x.</span>

<span class="sd">    This method uses a multivariate normal distribution to</span>
<span class="sd">    randomly select feature values. The sigma of the distribution</span>
<span class="sd">    is calculated over the training data to mimic the original</span>
<span class="sd">    distributions and a scaling factor is multiplied to</span>
<span class="sd">    adjust how large will be the neighborhood.</span>

<span class="sd">    It is possible to specify the number of generated samples</span>
<span class="sd">    by setting the size to a different value (default=100).</span>

<span class="sd">    Returns a matrix of shape (size, n_features) containing</span>
<span class="sd">    the sampled neighbors.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cov</span><span class="o">*</span><span class="n">factor</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">stability</span><span class="p">(</span>
    <span class="n">explain_single</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">neighborhood</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.01040152988347714
</pre></div></div>
</div>
</section>
<section id="Increasing-the-neighborhood-to-measure-stability">
<h2>Increasing the neighborhood to measure stability<a class="headerlink" href="#Increasing-the-neighborhood-to-measure-stability" title="Permalink to this headline">¶</a></h2>
<p>In the original work, the authors used fixed values for creating the neighborhood to evaluate stability.</p>
<p>I think this can be limiting in some situations. To verify, we will evaluate how stability changes when we increase the size of the neighborhood.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">factors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">axs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">factors</span><span class="p">,</span>
    <span class="p">[</span><span class="n">stability</span><span class="p">(</span>
        <span class="n">explain_single</span><span class="p">,</span>
        <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">neighborhood</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">factor</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="p">)</span> <span class="k">for</span> <span class="n">factor</span> <span class="ow">in</span> <span class="n">factors</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">axs</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Stability of explanation (smaller is better)&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Stability&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Multiplying factor&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/_explanations_stability_16_0.png" src="_images/_explanations_stability_16_0.png" />
</div>
</div>
</section>
<section id="The-Jaccard-Index">
<h2>The Jaccard Index<a class="headerlink" href="#The-Jaccard-Index" title="Permalink to this headline">¶</a></h2>
<p>In <em>Zhengze Zhou, Giles Hooker, Fei Wang. “S-LIME: Stabilized-LIME for Model Explanation”. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’21), August 14–18, 2021, Virtual Event, Singapore</em>, the authors used the Jaccrd index to evaluate the stability of a function.</p>
<div class="math">
<p><img src="_images/math/789046274f48d169d2b183f6de0dff6f99e1a5a0.png" alt="J(A, B) = \frac{|A \cap B|}{|A \cup B|}."/></p>
</div><p>This method ignores ordering, and is only concerned if the most important features will always be the same. Values close to 1 means that the algorithm is stable accros different iterations.</p>
<p>The Jaccard index is the ratio of the intersection over the union of two data sets. In the paper, to evaluate the stability for aa given test instance <img class="math" src="_images/math/907a4add6d5db5b7f197f7924f1371b8ac404fe6.png" alt="t"/>, they: 1. Execute the explanation method 20 times; 2. Saved the <img class="math" src="_images/math/9630132210b904754c9ab272b61cb527d12263ca.png" alt="k"/> most important features (with <img class="math" src="_images/math/c73f753c94530ca49cc163c3c37a353eb17fedba.png" alt="k=\{1, 2, 3, 4, 5\}"/>); 3. For each value of <img class="math" src="_images/math/10394fb9a1e2539ad64521b5d93d822aa0775a36.png" alt="k_i"/>, the <img class="math" src="_images/math/10394fb9a1e2539ad64521b5d93d822aa0775a36.png" alt="k_i"/> most important features for each execution were used to calculate the Jaccard index across all possible pairs (20*19 pairs), then the average was reported.</p>
<p>Their result look like this:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 3%" />
<col style="width: 97%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>k</p></th>
<th class="head"><p>Jaccard Index for the method</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>0.8</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0.7</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>0.0</p></td>
</tr>
</tbody>
</table>
<section id="Adaptating-the-Jaccard-Index">
<h3>Adaptating the Jaccard Index<a class="headerlink" href="#Adaptating-the-Jaccard-Index" title="Permalink to this headline">¶</a></h3>
<p>In the cell below the Jaccard Index is used to evaluate stability, but since the Partial Effects explanation method is deterministic, the calculation will be performed on the neighborhood of the test point. We can use <code class="docutils literal notranslate"><span class="pre">explain_single</span></code> and <code class="docutils literal notranslate"><span class="pre">neighborhood</span></code> functions previously declared.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">jaccard_index</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Method to calculate the ratio of the intersection</span>
<span class="sd">    over the union of two sets. This is known as Jaccard</span>
<span class="sd">    index and ranges from 0 to 1, measuring how simmilar</span>
<span class="sd">    the two sets are. A value equals to 1 means that the</span>
<span class="sd">    sets are identical (remembering that sets does not</span>
<span class="sd">    have order relations between its elements), and a</span>
<span class="sd">    value equals to 0 means that they are completely</span>
<span class="sd">    different.</span>

<span class="sd">    Takes as argument two python built-in sets A and B.</span>

<span class="sd">    Returns a float number representing the Jaccard Index.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">B</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">B</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">get_k_most_important</span><span class="p">(</span><span class="n">explanation</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Method that takes an array of explanation and</span>
<span class="sd">    returns the index of the k most important (highest)</span>
<span class="sd">    values in the array.</span>

<span class="sd">    Takes an array of explanations of shape (n_features, )</span>
<span class="sd">    and an integer k representing the size of the subset,</span>
<span class="sd">    k &lt;= len(explanations).</span>

<span class="sd">    Returns a python built-in set containing the indexes</span>
<span class="sd">    of the k highest values.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Reversing the order so its in descending order</span>
    <span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">explanation</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>


    <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="n">order</span><span class="p">[:</span><span class="n">k</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">jaccard_stability</span><span class="p">(</span><span class="n">explainer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">neighborhood</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Jaccard adaptation Stability function.</span>

<span class="sd">    Takes as argument an explanation method, a single observation</span>
<span class="sd">    x of shape (n_features, ), the neighborhood as a matrix of</span>
<span class="sd">    shape (n_neighbors, n_features), and the size of the subset being</span>
<span class="sd">    considered k</span>

<span class="sd">    Returns the mean Jaccard Index between the original sample</span>
<span class="sd">    and all neighbors, considering how similar the k most important</span>
<span class="sd">    subset of features between the explanation of the original data</span>
<span class="sd">    and its neighbors.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">original_explanation</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">original_jaccard_set</span> <span class="o">=</span> <span class="n">get_k_most_important</span><span class="p">(</span><span class="n">original_explanation</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">([</span>
        <span class="n">jaccard_index</span><span class="p">(</span>
            <span class="n">get_k_most_important</span><span class="p">(</span><span class="n">explainer</span><span class="p">(</span><span class="n">nb</span><span class="p">),</span> <span class="n">k</span><span class="p">),</span>
            <span class="n">original_jaccard_set</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">nb</span> <span class="ow">in</span> <span class="n">neighborhood</span>
    <span class="p">])</span>
</pre></div>
</div>
</div>
<p>In te cell below we create a table similar to the one reported in the S-LIME paper. For different multiplication factors that adjusts the size of the neighborhood, we can see how stable the explanations are in terms of returning the same most important features.</p>
<p>Notice that the maximum number of features is 4.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">fs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>

<span class="n">jaccard_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span>   <span class="o">=</span> <span class="p">[[</span><span class="n">jaccard_stability</span><span class="p">(</span><span class="n">explain_single</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">neighborhood</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">f</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fs</span><span class="p">],</span>
    <span class="n">index</span>   <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;factor </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fs</span><span class="p">],</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">jaccard_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k=1</th>
      <th>k=2</th>
      <th>k=3</th>
      <th>k=4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>factor 0.001</th>
      <td>1.00</td>
      <td>0.746667</td>
      <td>1.000</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>factor 0.01</th>
      <td>1.00</td>
      <td>0.713333</td>
      <td>1.000</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>factor 0.1</th>
      <td>1.00</td>
      <td>0.660000</td>
      <td>1.000</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>factor 1.0</th>
      <td>0.95</td>
      <td>0.673333</td>
      <td>0.945</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="_integrating_with_scikits_classes.html" class="btn btn-neutral float-right" title="Integrating with the Scikit: Pipeline and Gridsearch" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="_agnostic_explainers.html" class="btn btn-neutral float-left" title="Using agnostic explainers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Guilherme Aldeia.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>